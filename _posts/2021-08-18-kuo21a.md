---
title: Learning to Continually Learn Rapidly from Few and Noisy Data
abstract: 'Neural networks suffer from catastrophic forgetting and are unable to sequentially
  learn new tasks without guaranteed stationarity in data distribution. Continual
  learning could be achieved via replay â€“ by concurrently training externally stored
  old data while learning a new task. However, replay becomes less effective when
  each past task is allocated with less memory. To overcome this difficulty, we supplemented
  replay mechanics with meta-learning for rapid knowledge acquisition. By employing
  a meta-learner, which learns a learning rate per parameter per past task, we found
  that base learners produced strong results when less memory was available. Additionally,
  our approach inherited several meta-learning advantages for continual learning:
  it demonstrated strong robustness to continually learn under the presence of noises
  and yielded base learners to higher accuracy in less updates.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kuo21a
month: 0
tex_title: Learning to Continually Learn Rapidly from Few and Noisy Data
firstpage: 65
lastpage: 76
page: 65-76
order: 65
cycles: false
bibtex_author: Kuo, Nicholas I-Hsien and Harandi, Mehrtash and Fourrier, Nicolas and
  Walder, Christian and Ferraro, Gabriela and Suominen, Hanna
author:
- given: Nicholas I-Hsien
  family: Kuo
- given: Mehrtash
  family: Harandi
- given: Nicolas
  family: Fourrier
- given: Christian
  family: Walder
- given: Gabriela
  family: Ferraro
- given: Hanna
  family: Suominen
date: 2021-08-18
address:
container-title: AAAI Workshop on Meta-Learning and MetaDL Challenge
volume: '140'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 8
  - 18
pdf: http://proceedings.mlr.press/v140/kuo21a/kuo21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
