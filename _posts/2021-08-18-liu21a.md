---
title: Asymptotic Analysis of Meta-learning as a Recommendation Problem
abstract: 'Meta-learning tackles various means of learning from past tasks to perform
  new tasks better. In this paper, we focus on one particular statement of meta-learning:
  learning to recommend algorithms. We focus on a finite number of algorithms, which
  can be executed on tasks drawn i.i.d. according to a “meta-distribution”. We are
  interested in generalization performance of meta-predict strategies, i.e., the expected
  algorithm performances on new tasks drawn from the same meta-distribution. Assuming
  the perfect knowledge of the meta-distribution (i.e., in the limit of a very large
  number of training tasks), we ask ourselves under which conditions algorithm recommendation
  can benefit from meta-learning, and thus, in some sense, “defeat” the No-Free-Lunch
  theorem. We analyze four meta-predict strategies: Random, Mean, Greedy and Optimal.
  We identify optimality conditions for such strategies. We also define a notion of
  meta-learning complexity as the cardinal of the minimal clique of complementary
  algorithms. We illustrate our findings on experiments conducted on artificial and
  real data.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21a
month: 0
tex_title: Asymptotic Analysis of Meta-learning as a Recommendation Problem
firstpage: 100
lastpage: 114
page: 100-114
order: 100
cycles: false
bibtex_author: Liu, Zhengying and Guyon, Isabelle
author:
- given: Zhengying
  family: Liu
- given: Isabelle
  family: Guyon
date: 2021-08-18
address:
container-title: AAAI Workshop on Meta-Learning and MetaDL Challenge
volume: '140'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 8
  - 18
pdf: http://proceedings.mlr.press/v140/liu21a/liu21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
