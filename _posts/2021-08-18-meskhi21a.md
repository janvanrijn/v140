---
title: Learning Abstract Task Representations
abstract: A proper form of data characterization can guide the process of learning-algorithm
  selection and model-performance estimation. The field of meta-learning has provided
  a rich body of work describing effective forms of data characterization using different
  families of meta-features (statistical, model-based, information-theoretic, topological,
  etc.). In this paper, we start with the abundant set of existing meta-features and
  propose a method to induce new abstract meta-features as latent variables in a deep
  neural network. We discuss the pitfalls of using traditional meta-features directly
  and argue for the importance of learning high-level task properties. We demonstrate
  our methodology using a deep neural network as a feature extractor. We demonstrate
  that 1) induced meta-models mapping abstract meta-features to generalization metrics
  outperform other methods by $\~ 18%$ on average, and 2) abstract meta-features attain
  high feature-relevance scores.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: meskhi21a
month: 0
tex_title: Learning Abstract Task Representations
firstpage: 127
lastpage: 137
page: 127-137
order: 127
cycles: false
bibtex_author: Meskhi, Mikhail M. and Rivolli, Adriano and Mantovani, Rafael G. and
  Vilalta, Ricardo
author:
- given: Mikhail M.
  family: Meskhi
- given: Adriano
  family: Rivolli
- given: Rafael G.
  family: Mantovani
- given: Ricardo
  family: Vilalta
date: 2021-08-18
address:
container-title: AAAI Workshop on Meta-Learning and MetaDL Challenge
volume: '140'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 8
  - 18
pdf: http://proceedings.mlr.press/v140/meskhi21a/meskhi21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
